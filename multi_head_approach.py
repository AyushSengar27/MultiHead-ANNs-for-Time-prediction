# -*- coding: utf-8 -*-
"""Multi-Head Approach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXdWRCg7FoMwqFM1i3Mj1-ww0u56rT6m
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from tensorflow.keras import backend as K


IMAGE_WIDTH = 75
IMAGE_HEIGHT = 75
BATCH_SIZE = 32
NUM_EPOCHS = 100
NUM_CLASSES_HOURS = 12
NUM_CLASSES_MINUTES = 60
TOTAL_IMAGES = 18000
TEST_SIZE_RATIO = 0.2

# Load your datasets
images = np.load('/content/images.npy')
labels = np.load('/content/labels.npy')


if images.ndim == 3 and images.shape[0] == IMAGE_HEIGHT:

    images = images.reshape((TOTAL_IMAGES, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
elif images.ndim == 3 and images.shape[0] != IMAGE_HEIGHT:

    images = images.reshape((images.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, 1))


print("Corrected train images shape:", images.shape)


def preprocess_images(image_set):
    image_set = tf.image.resize(image_set, [IMAGE_WIDTH, IMAGE_HEIGHT])
    image_set = image_set / 255.0
    return image_set

def preprocess_labels(label_set):
    hours = label_set[:, 0]
    minutes = label_set[:, 1] / 59.0  # Scale minutes to 0-1
    return {'hours_output': hours, 'minutes_output': minutes}


# Common sense error metric
def common_sense_error_scaled(y_true_minutes, y_pred_minutes):

    pred_minutes = y_pred_minutes * 59.0
    true_minutes = y_true_minutes * 59.0

    # Compute the absolute time differences
    time_diff = tf.abs(true_minutes - pred_minutes)

    time_diff = tf.minimum(time_diff, 60.0 - time_diff)  # 60 minutes in an hour

    return tf.reduce_mean(time_diff)



# Split the data
train_images, test_images, train_labels, test_labels = train_test_split(
    images, labels, test_size=TEST_SIZE_RATIO, random_state=42)

# Preprocess the data
train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)
train_labels = preprocess_labels(train_labels)
test_labels = preprocess_labels(test_labels)
print("Train images shape:", train_images.shape)
print("Train labels (hours) shape:", train_labels['hours_output'].shape)
print("Train labels (minutes) shape:", train_labels['minutes_output'].shape)


# Building the Keras Model
def build_multitask_keras_model():
    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1))
    x = layers.Conv2D(64, (5, 5), padding='same', activation='relu')(inputs)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (5, 5), padding='same', activation='relu')(x)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Flatten()(x)
    x = layers.Dense(384, activation='relu')(x)
    x = layers.Dense(192, activation='relu')(x)
    outputs_hours = layers.Dense(NUM_CLASSES_HOURS, activation='softmax', name='hours_output')(x)
    outputs_minutes = layers.Dense(1, activation='linear', name='minutes_output')(x)
    model = models.Model(inputs=inputs, outputs=[outputs_hours, outputs_minutes])
    return model

# Callbacks
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10)
model_checkpoint = callbacks.ModelCheckpoint(
    'best_model.h5', save_best_only=True, monitor='val_loss')

# Create and compile the model
multitask_model = build_multitask_keras_model()

multitask_model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-4),
    loss={
        'hours_output': 'sparse_categorical_crossentropy',
        'minutes_output': 'mean_squared_error'
    },
    metrics={
        'hours_output': ['accuracy'],
        'minutes_output': ['mse']
    }
)





train_dataset = tf.data.Dataset.from_tensor_slices(
    (train_images, {'hours_output': train_labels['hours_output'], 'minutes_output': train_labels['minutes_output']})
).shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)

test_dataset = tf.data.Dataset.from_tensor_slices(
    (test_images, {'hours_output': test_labels['hours_output'], 'minutes_output': test_labels['minutes_output']})
).batch(BATCH_SIZE)


history = multitask_model.fit(
    train_dataset,
    epochs=NUM_EPOCHS,
    validation_data=test_dataset,
    callbacks=[early_stopping, model_checkpoint]
)


# Load the best model
multitask_model.load_weights('best_model.h5')

def evaluate_common_sense_error_scaled(model, test_dataset):

    hour_predictions, minute_predictions_scaled = model.predict(test_dataset)
    hour_predictions = np.argmax(hour_predictions, axis=1)


    minute_predictions = minute_predictions_scaled * 59.0


    true_hours = np.concatenate([y['hours_output'] for x, y in test_dataset], axis=0)
    true_minutes_scaled = np.concatenate([y['minutes_output'] for x, y in test_dataset], axis=0)
    true_minutes = true_minutes_scaled * 59.0

    total_true_minutes = true_hours * 60 + true_minutes
    total_pred_minutes = hour_predictions * 60 + minute_predictions
    time_diff = np.abs(total_true_minutes - total_pred_minutes)
    time_diff = np.minimum(time_diff, 720 - time_diff)  # 720 minutes in 12 hours

    return np.mean(time_diff)

# Evaluate the common sense error on the test set with scaled values
common_sense_error_scaled = evaluate_common_sense_error_scaled(multitask_model, test_dataset)
print(f"Common sense error on the test set (scaled minutes): {common_sense_error_scaled:.2f} minutes")

"""Deeper Network"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from tensorflow.keras import backend as K

# Constants
IMAGE_WIDTH = 75
IMAGE_HEIGHT = 75
BATCH_SIZE = 32
NUM_EPOCHS = 100
NUM_CLASSES_HOURS = 12
NUM_CLASSES_MINUTES = 60
TOTAL_IMAGES = 18000
TEST_SIZE_RATIO = 0.2


images = np.load('/content/images.npy')
labels = np.load('/content/labels.npy')

if images.ndim == 3 and images.shape[0] == IMAGE_HEIGHT:

    images = images.reshape((TOTAL_IMAGES, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
elif images.ndim == 3 and images.shape[0] != IMAGE_HEIGHT:

    images = images.reshape((images.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, 1))


print("Corrected train images shape:", images.shape)


# Preprocessing functions
def preprocess_images(image_set):
    image_set = tf.image.resize(image_set, [IMAGE_WIDTH, IMAGE_HEIGHT])
    image_set = image_set / 255.0
    return image_set

def preprocess_labels(label_set):
    hours = label_set[:, 0]
    minutes = label_set[:, 1] / 59.0  # Scale minutes to 0-1
    return {'hours_output': hours, 'minutes_output': minutes}



def common_sense_error_scaled(y_true_minutes, y_pred_minutes):

    pred_minutes = y_pred_minutes * 59.0
    true_minutes = y_true_minutes * 59.0


    time_diff = tf.abs(true_minutes - pred_minutes)


    time_diff = tf.minimum(time_diff, 60.0 - time_diff)  # 60 minutes in an hour

    return tf.reduce_mean(time_diff)



# Split the data
train_images, test_images, train_labels, test_labels = train_test_split(
    images, labels, test_size=TEST_SIZE_RATIO, random_state=42)

# Preprocess the data
train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)
train_labels = preprocess_labels(train_labels)
test_labels = preprocess_labels(test_labels)
print("Train images shape:", train_images.shape)
print("Train labels (hours) shape:", train_labels['hours_output'].shape)
print("Train labels (minutes) shape:", train_labels['minutes_output'].shape)


# Building the Keras Model
def build_multitask_keras_model():
# Input layer
    inputs = layers.Input(shape=(75, 75, 1))

    # First conv block
    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='valid', activation='relu')(inputs)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Second conv block
    x = layers.Conv2D(128, (3, 3), padding='valid', activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Third conv block
    x = layers.Conv2D(256, (3, 3), padding='valid', activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Fourth conv block
    x = layers.Conv2D(512, (3, 3), padding='valid', activation='relu')(x)
    x = layers.Flatten()(x)

    # Dense layers before final outputs
    # Two branches out from the last convolution
    hour_branch = layers.Dense(144, activation='relu')(x)
    minute_branch = layers.Dense(200, activation='relu')(x)

    # Final output layers
    hour_branch = layers.Dense(144, activation='relu')(hour_branch)
    minute_branch = layers.Dense(100, activation='relu')(minute_branch)

    hours_output = layers.Dense(12, activation='softmax', name='hours_output')(hour_branch)
    minutes_output = layers.Dense(1, activation='linear', name='minutes_output')(minute_branch)

    # Create model
    model = models.Model(inputs=inputs, outputs=[hours_output, minutes_output])

    return model

# Callbacks
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10)
model_checkpoint = callbacks.ModelCheckpoint(
    'best_model.h5', save_best_only=True, monitor='val_loss')

# Create and compile the model
multitask_model = build_multitask_keras_model()
# Compile the model with the custom metric for each output
multitask_model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-4),
    loss={
        'hours_output': 'sparse_categorical_crossentropy',
        'minutes_output': 'mean_squared_error'
    },
    metrics={
        'hours_output': ['accuracy'],
        'minutes_output': ['mse']
    }
)





train_dataset = tf.data.Dataset.from_tensor_slices(
    (train_images, {'hours_output': train_labels['hours_output'], 'minutes_output': train_labels['minutes_output']})
).shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)

test_dataset = tf.data.Dataset.from_tensor_slices(
    (test_images, {'hours_output': test_labels['hours_output'], 'minutes_output': test_labels['minutes_output']})
).batch(BATCH_SIZE)


history = multitask_model.fit(
    train_dataset,
    epochs=NUM_EPOCHS,
    validation_data=test_dataset,
    callbacks=[early_stopping, model_checkpoint]
)


# Load the best model
multitask_model.load_weights('best_model.h5')

def evaluate_common_sense_error_scaled(model, test_dataset):

    hour_predictions, minute_predictions_scaled = model.predict(test_dataset)
    hour_predictions = np.argmax(hour_predictions, axis=1)


    minute_predictions = minute_predictions_scaled * 59.0


    true_hours = np.concatenate([y['hours_output'] for x, y in test_dataset], axis=0)
    true_minutes_scaled = np.concatenate([y['minutes_output'] for x, y in test_dataset], axis=0)
    true_minutes = true_minutes_scaled * 59.0


    total_true_minutes = true_hours * 60 + true_minutes
    total_pred_minutes = hour_predictions * 60 + minute_predictions
    time_diff = np.abs(total_true_minutes - total_pred_minutes)
    time_diff = np.minimum(time_diff, 720 - time_diff)  # 720 minutes in 12 hours

    return np.mean(time_diff)


common_sense_error_scaled = evaluate_common_sense_error_scaled(multitask_model, test_dataset)
print(f"Common sense error on the test set (scaled minutes): {common_sense_error_scaled:.2f} minutes")

"""Deeper 150*150"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from tensorflow.keras import backend as K
from keras.optimizers import RMSprop


IMAGE_WIDTH = 150
IMAGE_HEIGHT = 150
BATCH_SIZE = 32
NUM_EPOCHS = 100
NUM_CLASSES_HOURS = 12
NUM_CLASSES_MINUTES = 60
TOTAL_IMAGES = 18000
TEST_SIZE_RATIO = 0.2

# Load your datasets
images = np.load("drive/My Drive/images.npy")
labels = np.load("drive/My Drive/labels.npy")


if images.ndim == 3 and images.shape[0] == IMAGE_HEIGHT:

    images = images.reshape((TOTAL_IMAGES, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
elif images.ndim == 3 and images.shape[0] != IMAGE_HEIGHT:

    images = images.reshape((images.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, 1))

print("Corrected train images shape:", images.shape)


def preprocess_images(image_set):
    image_set = tf.image.resize(image_set, [IMAGE_WIDTH, IMAGE_HEIGHT])
    image_set = image_set / 255.0
    return image_set

def preprocess_labels(label_set):
    hours = label_set[:, 0]
    minutes = label_set[:, 1] / 59.0  # Scale minutes to 0-1
    return {'hours_output': hours, 'minutes_output': minutes}



def common_sense_error_scaled(y_true_minutes, y_pred_minutes):

    pred_minutes = y_pred_minutes * 59.0
    true_minutes = y_true_minutes * 59.0


    time_diff = tf.abs(true_minutes - pred_minutes)


    time_diff = tf.minimum(time_diff, 60.0 - time_diff)

    return tf.reduce_mean(time_diff)



# Split the data
train_images, test_images, train_labels, test_labels = train_test_split(
    images, labels, test_size=TEST_SIZE_RATIO, random_state=42)

# Preprocess the data
train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)
train_labels = preprocess_labels(train_labels)
test_labels = preprocess_labels(test_labels)
print("Train images shape:", train_images.shape)
print("Train labels (hours) shape:", train_labels['hours_output'].shape)
print("Train labels (minutes) shape:", train_labels['minutes_output'].shape)



def build_multitask_keras_model():

    inputs = layers.Input(shape=(150, 150, 1))

    # First conv block
    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='valid', activation='relu')(inputs)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Second conv block
    x = layers.Conv2D(128, (3, 3), padding='valid', activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Third conv block
    x = layers.Conv2D(256, (3, 3), padding='valid', activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.BatchNormalization()(x)

    # Fourth conv block
    x = layers.Conv2D(512, (3, 3), padding='valid', activation='relu')(x)
    x = layers.Flatten()(x)

    # Dense layers before final outputs
    # Two branches out from the last convolution
    hour_branch = layers.Dense(144, activation='relu')(x)
    minute_branch = layers.Dense(200, activation='relu')(x)

    # Final output layers
    hour_branch = layers.Dense(144, activation='relu')(hour_branch)
    minute_branch = layers.Dense(100, activation='relu')(minute_branch)

    hours_output = layers.Dense(12, activation='softmax', name='hours_output')(hour_branch)
    minutes_output = layers.Dense(1, activation='linear', name='minutes_output')(minute_branch)

    # Create model
    model = models.Model(inputs=inputs, outputs=[hours_output, minutes_output])

    return model

# Callbacks
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10)
model_checkpoint = callbacks.ModelCheckpoint(
    'best_model.h5', save_best_only=True, monitor='val_loss')

# Create and compile the model
multitask_model = build_multitask_keras_model()

multitask_model.compile(
    optimizer=RMSprop(),
    loss={
        'hours_output': 'sparse_categorical_crossentropy',
        'minutes_output': 'mean_squared_error'
    },
    metrics={
        'hours_output': ['accuracy'],
        'minutes_output': ['mse']
    }
)




train_dataset = tf.data.Dataset.from_tensor_slices(
    (train_images, {'hours_output': train_labels['hours_output'], 'minutes_output': train_labels['minutes_output']})
).shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)

test_dataset = tf.data.Dataset.from_tensor_slices(
    (test_images, {'hours_output': test_labels['hours_output'], 'minutes_output': test_labels['minutes_output']})
).batch(BATCH_SIZE)


history = multitask_model.fit(
    train_dataset,
    epochs=NUM_EPOCHS,
    validation_data=test_dataset,
    callbacks=[early_stopping, model_checkpoint]
)


# Load the best model
multitask_model.load_weights('best_model.h5')

def evaluate_common_sense_error_scaled(model, test_dataset):

    hour_predictions, minute_predictions_scaled = model.predict(test_dataset)
    hour_predictions = np.argmax(hour_predictions, axis=1)


    minute_predictions = minute_predictions_scaled * 59.0


    true_hours = np.concatenate([y['hours_output'] for x, y in test_dataset], axis=0)
    true_minutes_scaled = np.concatenate([y['minutes_output'] for x, y in test_dataset], axis=0)
    true_minutes = true_minutes_scaled * 59.0


    total_true_minutes = true_hours * 60 + true_minutes
    total_pred_minutes = hour_predictions * 60 + minute_predictions
    time_diff = np.abs(total_true_minutes - total_pred_minutes)
    time_diff = np.minimum(time_diff, 720 - time_diff)  # 720 minutes in 12 hours

    return np.mean(time_diff)

# Evaluate the common sense error on the test set with scaled values
common_sense_error_scaled = evaluate_common_sense_error_scaled(multitask_model, test_dataset)
print(f"Common sense error on the test set (scaled minutes): {common_sense_error_scaled:.2f} minutes")

